\section{Επεξήγηση κώδικα}
Έχουν γίνει κάποιες προσθήκες ώστε να χρησιμοποιούνται OpenMP threads στους υπολογισμούς. Συγκεκριμένα, στην συνάρτηση \mintinline{C}{Initial_state} έχει προστεθεί η εξής εντολή πριν το διπλό \mintinline{C}{for}, η οποία κάνει collapse το διπλό \mintinline{C}{for} ώστε να κατανεμηθούν οι υπολογισμοί σε πολλά νήματα. \\
\begin{tcolorbox}
\mintinline{C}{#pragma}\mintinline{C}{ omp parallel for collapse(2) private(i, j) schedule(static)}
\end{tcolorbox}
Οι συναρτήσεις \mintinline{C}{Next_generation_inner} και \mintinline{C}{Next_generation_outer} πλέον επιστρέφουν \mintinline{C}{int} αντί για \mintinline{C}{void}. Αυτό συμβαίνει, ώστε όλα τα νήματα της διεργασίας να υπολογίζουν εάν έχουν γίνει αλλαγές στο πλέγμα χρησιμοποιώντας μία ιδιωτική για κάθε νήμα μεταβλητή (η \mintinline{C}{char same} όπως φαίνεται παρακάτω). Όλες οι ιδιωτικές μεταβλητές γίνονται reduce στο τέλος, και το άθροισμα που προκύπτει είναι αυτό που επιστρέφει η συνάρτηση, όπως φαίνεται στο παρακάτω απόσπασμα της \mintinline{C}{Next_generation_inner}.
\clearpage
\begin{tcolorbox}
\begin{minted}{C}
int neighbors, i, j, lsum = 0;
char same = 1;

#pragma omp parallel
{
    #pragma omp for collapse(2) private(i, j, same) schedule(static)
    for (i = 2; i < rows - 2; i++)
    {
        for (j = 2; j < columns - 2; j++)
        {
         .
         .
         .
            same = same && (*(life + i * columns + j) == *(life_copy + i * columns + j));
        }
    }

    #pragma omp reduction(+:lsum)
    {
        lsum += same;
    }
}
return lsum;       
\end{minted}
\end{tcolorbox}
Αντίστοιχα, στην συνάρτηση \mintinline{C}{Next_generation_outer} έχουν προστεθεί τέσσερις νέες εντολές, μία πριν από κάθε \mintinline{C}{for} που υπολογίζει άλω σειρά ή στήλη. Όπως και η \mintinline{C}{Next_generation_inner}, έτσι και εδώ γίνονται reduce όλες οι same των νημάτων σε μία μεταβλητή η οποία στο τέλος της συνάρτησης επιστρέφεται. \\
\begin{tcolorbox}
\begin{minted}{C}
#pragma omp parallel
{
    /* Upper row */
    #pragma omp for private(i, same) schedule(static)
    for (i = 1; i < columns - 1; i++)
    {
    .
    .
    .
        same = same && (*(life + columns + i) == *(life_copy + columns + i));
    .
    .
    .
    #pragma omp reduction(+:lsum)
    {
        lsum += same;
    }
}
return lsum;
\end{minted}
\end{tcolorbox}

\section[Βέλτιστος συνδυασμός]{Βέλτιστος συνδυασμός διεργασιών--νημάτων}
Ακολουθεί πίνακας με τα αποτελέσματα των διάφορων συνδυασμών διεργασιών -- νημάτων. Οι δοκιμές έγιναν για μέγεθος πίνακα 3360 και 2000 επαναλήψεις, με timeout ενός λεπτού.
\begin{table}[H]
\centering
\begin{tabular}{| c | c | }
\hline
Διεργασίες--νήματα & Χρόνος \\
\hline
1--1 & Timeout \\
1--2 & Timeout \\
1--4 & Timeout \\
1--8 & Timeout \\
1--16 & Timeout \\
2--4 & Timeout \\
2--8 & Timeout \\
2--16 & Timeout \\
\textbf{4--2} & \textbf{23.89} \\
4--4 & 52.89 \\
4--8 & 33.31 \\
4--16 & 34.79 \\
8--2 & 49.03 \\
8--4 & Timeout \\
8--8 & 25.61 \\
8--16 & 28.48 \\
\hline
\end{tabular}
\caption{Συνδυασμοί διεργασιών -- νημάτων σε έναν κόμβο, και οι αντίστοιχοι χρόνοι εκτέλεσης. Ο συνδυασμός 4--2 προσφέρει το καλύτερο αποτέλεσμα.}
\label{tab:OpenMPOneNode}
\end{table}

Οι καλύτερες επιδόσεις επετεύχθησαν με 4 διεργασίες και 2 νήματα ανά διεργασία. Επομένως, οι μετρήσεις στην Αργώ θα γίνουν με αυτές τις τιμές.

\section{Μετρήσεις στην Αργώ}
\begin{multicols}{2}
Για τις μετρήσεις στην Αργώ\footnote{Τα αποτελέσματα μπορούν να βρεθούν στο directory \path{/home/pool/argo029/GameOfLife/MPI_OpenMP/Results}}, επιλέγουμε συνδυασμούς διεργασιών--νημάτων από τους οποίους προκύπτουν τέλεια τετράγωνα. Επειδή βρήκαμε ότι οι αριθμοί διεργασιών--νημάτων που προσφέρουν τις καλύτερες μετρήσεις είναι και οι δύο άρτιοι (4 και 2 αντίστοιχα), δε γίνεται να κάνουμε δοκιμές για τα περιττά τετράγωνα 9, 25, 49. Επίσης, δεν θα πάρουμε ξανά μετρήσεις για 1 διεργασία με 1 νήμα, αφού το αποτέλεσμα είναι το ίδιο με αυτό στα καθαρά MPI. Για την περίπτωση αυτή, θα χρησιμοποιήσουμε ξανά τους χρόνους που δίνονται στον πίνακα \ref{tab:timesMPIAllreduce}. Επίσης, αυτή την φορά δεν θα επιτρέψουμε να τρέχει η κάθε μέτρηση παραπάνω από ένα λεπτό, προκειμένου να αποφευχθεί ο συνωστισμός στην Αργώ. Εάν ξεπεραστεί το όριο, στον πίνακα θα υπάρχει απλώς η ένδειξη ``Timeout''. Φυσικά, όπου παρατηρηθεί επιβράδυνση, σταματούν οι μετρήσεις και τα υπόλοιπα κελιά της ίδιας σειράς παίρνουν την τιμή «Ε».
\end{multicols}

\begin{table}[H]
\centering
\small
\begin{tabular}{| l | c | c | c | c | c | c | c | c | c | c |}
\hline
\diagbox{Μέγεθος}{Διεργασίες * Νήματα} & 1 & 4 & 16 & 36 & 64 & 80 \\
\hline
840 & 11.62 & 5.96 & 0.82 & 0.63 & 0.56 & 0.57 \\
\hline
1680 & 46.41 & 23.54 & 3.23 & 1.69 & 0.99 & 0.92 \\
\hline
3360 & 186.24 & Timeout & 12.09 & 5.63 & 3.38 & 2.83 \\
\hline
6720 & 743.67 & Timeout & 48.72 & 21.63 & 12.38 & 10.07 \\
\hline
13440 & 2972.15 & Timeout & Timeout & Timeout & 49.06 & 39.51 \\
\hline
\end{tabular}
\caption{Χρόνοι για υβριδικά MPI--OpenMP}
\label{tab:timesMPIOpenMP}
\end{table}

\begin{table}[H]
\centering
\small
\begin{tabular}{| l | c | c | c | c | c | c | c | c | c | c |}
\hline
\diagbox{Μέγεθος}{Διεργασίες * Νήματα} & 1 & 4 & 16 & 36 & 64 & 80 \\
\hline
840 & 1 & 1.94 & 14.17 & 18.44 & 20.75 & 20.38 \\
\hline
1680 & 1 & 1.97 & 14.36 & 27.46 & 46.87 & 50.44 \\
\hline
3360 & 1 & Timeout & 15.4 & 33.07 & 55.1 & 65.8 \\
\hline
6720 & 1 & Timeout & 15.26 & 34.38 & 60.07 & 73.85 \\
\hline
13440 & 1 & Timeout & Timeout & Timeout & 60.58 & 75.91 \\
\hline
\end{tabular}
\caption{Επιτάχυνση ($S = Ts / Tp$) για υβριδικά MPI--OpenMP}
\label{tab:speedupMPIOpenMP}
\end{table}

\begin{table}[H]
\centering
\small
\begin{tabular}{| l | c | c | c | c | c | c | c | c | c | c |}
\hline
\diagbox{Μέγεθος}{Διεργασίες * Νήματα} & 1 & 4 & 16 & 36 & 64 & 80 \\
\hline
840 & 1 & 0.48 & 0.88 & 0.51 & 0.32 & 0.25 \\
\hline
1680 & 1 & 0.49 & 0.89 & 0.76 & 0.73 & 0.63 \\
\hline
3360 & 1 & Timeout & 0.96 & 0.93 & 0.86 & 1.02 \\
\hline
6720 & 1 & Timeout & 0.95 & 0.95 & 0.93 & 0.92 \\
\hline
13440 & 1 & Timeout & Timeout & Timeout & 0.94 & 0.94 \\
\hline
\end{tabular}
\caption{Αποδοτικότητα ($E = S / ( p * thr)$) για υβριδικά MPI--OpenMP}
\label{tab:efficiencyMPIOpenMP}
\end{table}

Παραθέτουμε ξανά τον πίνακα \ref{tab:timesMPIAllreduce} για εύκολη σύγκριση με τα καθαρά MPI.

\begin{table}[H]
\begin{adjustwidth}{-0.5cm}{}
\centering
\small
\begin{tabular}{| l | c | c | c | c | c | c | c | c | c |}
\hline
\diagbox{Μέγεθος}{Διεργασίες} & 1 & 4 & 9 & 16 & 25 & 36 & 49 & 64 & 80\\
\hline
840 & 11.62 & 2.95 & 3.96 & Ε & Ε & Ε & Ε & Ε & Ε \\
\hline
1680 & 46.41 & 11.65 & 7.87 & 6.61 & 6.56 & 6.28 & 6.16 & 7.32 & Ε \\
\hline
3360 & 186.24 & 46.5 & 28.39 & 17.56 & 16.27 & 14.24 & 10.68 & 9.62 & 8.62 \\
\hline
6720 & 743.67 & 187.54 & 89.80 & 50.66 & 43.85 & 21.26 & 27.18 & 24.08 & 21.53 \\
\hline
13440 & 2972.15 & 746.77 & 334.29 & 193.03 & 130.33 & 94.43 & 62.76 & 53.5 & 43.85 \\
\hline
\end{tabular}
\caption{Χρόνοι για καθαρά MPI με Allreduce}
\label{tab:timesMPIAllreduce2}
\end{adjustwidth}
\end{table}
\clearpage
\section{Σχόλια και Παρατηρήσεις}
\begin{multicols}{2}
Με βάση τους πίνακες, παρατηρούμε ότι το OpenMP υπερτερεί σε σχέση με τα καθαρά MPI όταν έχουμε πολλές διεργασίες και νήματα, πχ 40 διεργασίες με 2 νήματα η καθεμία, καθώς και μεγάλο μέγεθος προβλήματος. Στα μικρότερα μεγέθη προβλημάτων συμπεριφέρεται επίσης καλύτερα, δεδομένου όμως ότι έχουμε ξανά πολλές διεργασίες και νήματα. Πχ, για μέγεθος 1680 και 64 διεργασίες, τα καθαρά MPI δίνουν χρόνο 7.32\si{\second}, ενώ το υβριδικό πρόγραμμα δίνει χρόνο 0.92\si{\second}. Για λιγότερες διεργασίες--νήματα όμως ($p * thr \leq 4$), είναι προτιμότερο να έχουμε καθαρά MPI όπως φαίνεται και στον πίνακα των χρόνων. Ενδεχομένως εδώ να «πληρώνουμε» το κόστος της δημιουργίας και καταστροφής των νημάτων σε κάθε γενιά. Παρόλα αυτά, το υβριδικό πρόγραμμα κλιμακώνει καλύτερα, και θα πρέπει να προτιμάται, εφόσον φυσικά υπάρχουν διαθέσιμοι πόροι. Ενδεχομένως με μία υλοποίηση όπου τα νήματα θα δημιουργούνται και θα καταστρέφονται μόνο μία φορά έξω από το κεντρικό loop, να είχαμε ακόμα καλύτερους χρόνους. Δυστυχώς, δεν κατάφερα να ολοκληρώσω μια τέτοια υλοποίηση. \par
Όσον αφορά το scalability, το πρόβλημα δεν είναι strongly scalable ούτε στην περίπτωση του υβριδικού προγράμματος, αφού η αποδοτικότητα φθίνει όσο αυξάνεται το πλήθος των νημάτων. Συγκρίνοντας όμως την αποδοτικότητα μεταξύ 4 και 16 νημάτων, ή 16 και 64 νημάτων, βλέπουμε πως η αποδοτικότητα μένει περίπου ίδια για κάθε διπλασιασμό πλευράς. Επομένως θα μπορούσαμε να πούμε πως το πρόβλημα είναι \emph{weakly scalable}, αν και καλό θα ήταν να γίνουν περισσότερες δοκιμές.
\end{multicols}